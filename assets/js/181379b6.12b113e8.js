"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4406],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>h});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),c=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=c(e.components);return a.createElement(s.Provider,{value:t},e.children)},d="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},g=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),d=c(n),g=r,h=d["".concat(s,".").concat(g)]||d[g]||p[g]||o;return n?a.createElement(h,i(i({ref:t},u),{},{components:n})):a.createElement(h,i({ref:t},u))}));function h(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=g;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[d]="string"==typeof e?e:r,i[1]=l;for(var c=2;c<o;c++)i[c]=n[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}g.displayName="MDXCreateElement"},25293:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>c,default:()=>m,frontMatter:()=>s,metadata:()=>u,toc:()=>p});var a=n(87462),r=(n(67294),n(3905));const o=n.p+"assets/images/web_crawl-fe3fa3d0b2b94658d3833b93c0a04f25.png",i=n.p+"assets/images/benefits-c183a0b67119a93e2de7cfbac13331cb.png",l=n.p+"assets/images/process-38e97ec8a536b5182307fcd51963e2a7.png",s={slug:"web-crawling",title:"Web Crawling, A Beginner\u2019s Perspective on Data Extraction",authors:["mahima"],tags:["web crawling","data extraction","automation"],description:"In this blog, you will explore the fundamentals of web crawling and how you can get started with your own data extraction projects.",keywords:["web crawling","BeautifulSoup","scrapy","data extraction"]},c=void 0,u={permalink:"/blog/web-crawling",source:"@site/blog/2023-10-08-web-crawling/index.mdx",title:"Web Crawling, A Beginner\u2019s Perspective on Data Extraction",description:"In this blog, you will explore the fundamentals of web crawling and how you can get started with your own data extraction projects.",date:"2023-10-08T00:00:00.000Z",formattedDate:"October 8, 2023",tags:[{label:"web crawling",permalink:"/blog/tags/web-crawling"},{label:"data extraction",permalink:"/blog/tags/data-extraction"},{label:"automation",permalink:"/blog/tags/automation"}],readingTime:5.105,hasTruncateMarker:!0,authors:[{name:"Mahima Churi",title:"COO @ TCET Open Source | Full Stack Web Developer",url:"https://github.com/Mahitej28",imageURL:"https://github.com/Mahitej28.png",key:"mahima"}],frontMatter:{slug:"web-crawling",title:"Web Crawling, A Beginner\u2019s Perspective on Data Extraction",authors:["mahima"],tags:["web crawling","data extraction","automation"],description:"In this blog, you will explore the fundamentals of web crawling and how you can get started with your own data extraction projects.",keywords:["web crawling","BeautifulSoup","scrapy","data extraction"]},prevItem:{title:"CNCF Zero to Merge - Your first step towards Open-Source",permalink:"/blog/CNCF-Zero-to-Merge"},nextItem:{title:"Blockchain - Future of Tommorow",permalink:"/blog/intro-to-blockchain"}},d={authorsImageUrls:[void 0]},p=[{value:"What if I told you that web crawling could come to your rescue even in unexpected work scenarios? \ud83e\udd14",id:"what-if-i-told-you-that-web-crawling-could-come-to-your-rescue-even-in-unexpected-work-scenarios-",level:2},{value:"Benefits",id:"benefits",level:2},{value:"Process of Web Crawling",id:"process-of-web-crawling",level:2},{value:"Getting Started with Web Crawling",id:"getting-started-with-web-crawling",level:2},{value:"Ethical Considerations",id:"ethical-considerations",level:2},{value:"Conclusion",id:"conclusion",level:2}],g={toc:p},h="wrapper";function m(e){let{components:t,...n}=e;return(0,r.kt)(h,(0,a.Z)({},g,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("em",{parentName:"p"},"Web crawling, also known as web scraping, is the process of automatically extracting data from websites.\nIt allows us to gather valuable information from various sources on the internet efficiently and in a structured manner.\nIn this blog, we\u2019ll explore the fundamentals of web crawling and how you can get started with your own data extraction projects."))),(0,r.kt)("br",null),(0,r.kt)("figure",null,(0,r.kt)("img",{src:o,style:{border:"2px solid grey"}})),(0,r.kt)("h2",{id:"what-if-i-told-you-that-web-crawling-could-come-to-your-rescue-even-in-unexpected-work-scenarios-"},"What if I told you that web crawling could come to your rescue even in unexpected work scenarios? \ud83e\udd14"),(0,r.kt)("p",null,"Imagine you\u2019re on a relaxing weekend, enjoying your favorite Netflix series, when suddenly your boss calls with an urgent task."),(0,r.kt)("p",null,"Let\u2019s say your boss needs a comprehensive analysis of competitors\u2019 pricing for an upcoming project. Manually collecting this data from various websites would be time-consuming and error-prone.\nHowever, with web crawling, you can ",(0,r.kt)("strong",{parentName:"p"},"automate the data extraction process"),", quickly gathering pricing information from multiple sources and generating a detailed report.\nNot only does this save you hours of manual work, but it also ",(0,r.kt)("strong",{parentName:"p"},"ensures accuracy")," and ",(0,r.kt)("strong",{parentName:"p"},"provides valuable insights")," for your boss."),(0,r.kt)("p",null,"Web crawling can be a game-changer in various work scenarios. Need to gather customer reviews for a product launch?\nWeb crawling can swiftly scrape reviews from e-commerce platforms, allowing you to analyze sentiment and make data-driven decisions.\nWant to monitor industry trends or track news updates? Web crawling can continuously fetch relevant information from news websites, keeping you up to date and enabling you to stay ahead of the competition."),(0,r.kt)("h2",{id:"benefits"},"Benefits"),(0,r.kt)("p",null,"Let's have a look at the various benefits of web crawling that have made it a popular concept for seamless integration within large-scale enterprises."),(0,r.kt)("figure",null,(0,r.kt)("img",{src:i,style:{border:"2px solid black"}}),(0,r.kt)("center",null,(0,r.kt)("figcaption",null,"Benefits of Web Crawling"))),(0,r.kt)("h2",{id:"process-of-web-crawling"},"Process of Web Crawling"),(0,r.kt)("figure",null,(0,r.kt)("img",{src:l,style:{border:"2px solid black"}}),(0,r.kt)("center",null,(0,r.kt)("figcaption",null,"Process of Web Crawling"))),(0,r.kt)("details",null,(0,r.kt)("summary",null,(0,r.kt)("b",null,"\ud83d\udca1 Discovery")),(0,r.kt)("div",null,"In the discovery stage, a web crawler starts by identifying set of seed URLs. These seed URLs are the starting points from which the crawler begins exploring the web. They can be manually provided or generated programmatically. The crawler then extracts the links present on the web page of the seed URLs and adds them to a queue for further processing.")),(0,r.kt)("details",null,(0,r.kt)("summary",null,(0,r.kt)("b",null,"\ud83d\udd77\ufe0f Crawling")),(0,r.kt)("div",null,"The crawling stage involves visiting the URLs in the queue and retrieving the corresponding web pages. The crawler sends HTTP requests to the web servers hosting the pages and receives HTTP responses in return. The responses typically include HTML content, but they can also include other types of files such as images, CSS files, or JavaScript files. The crawler parses the HTML content to extract links and other relevant information for subsequent crawling.")),(0,r.kt)("details",null,(0,r.kt)("summary",null,(0,r.kt)("b",null,"\u26cf\ufe0f Fetching")),(0,r.kt)("div",null,"During the fetching stage, the crawler retrieves the content of the web pages by downloading them from the web servers. This process involves downloading the HTML and any associated files, such as images or scripts, required to render the page correctly. The fetched content is then stored for further processing.")),(0,r.kt)("details",null,(0,r.kt)("summary",null,(0,r.kt)("b",null,"\ud83d\udcbb Rendering")),(0,r.kt)("div",null,"Rendering refers to the process of processing and executing JavaScript code present on web pages. Some web pages heavily rely on JavaScript to load and display content dynamically. Modern web crawlers often include a rendering engine that can execute JavaScript code, allowing the crawler to handle pages that rely on client-side rendering.")),(0,r.kt)("details",null,(0,r.kt)("summary",null,(0,r.kt)("b",null,"\ud83d\udcd1 Indexing")),(0,r.kt)("div",null,"Once the web pages are fetched and rendered, the crawler can extract the desired data from the pages. This data can include text content, metadata, links, or any other relevant information. The extracted data is typically processed and stored in an organized manner, such as in a database or an index, for further analysis or retrieval.")),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"It\u2019s important to note that web crawling is an iterative process.\nAs the crawler discovers new links during the crawling stage, it adds them to the queue for subsequent crawling, continuing the process of discovery, crawling, fetching, rendering, and indexing for a broader coverage of the web.")),(0,r.kt)("h2",{id:"getting-started-with-web-crawling"},"Getting Started with Web Crawling"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Identify Your Data Needs"),": Determine the specific information you want to extract from websites.\nIt could be product details, contact information, news articles, or any other relevant data.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Choose a Web Crawling Tool"),": There are various web crawling frameworks and libraries available, such as BeautifulSoup and Scrapy in Python.\nSelect a tool that aligns with your programming language and project requirements."))),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("em",{parentName:"p"},"You can learn more about Python Scrapy ",(0,r.kt)("a",{parentName:"em",href:"https://docs.scrapy.org/en/latest/intro/tutorial.html"},(0,r.kt)("strong",{parentName:"a"},"here"))))),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Understand the Website Structure"),": Familiarize yourself with the target website\u2019s structure. Identify the HTML elements that contain the data you need, such as class names, IDs, or specific tags.\nSome key steps to follow here may include:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre"},"1. Inspect the web page\n\n2. Explore the HTML Elements\n\n3. Identify unique Identifiers\n")))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-html"},'Example: <div class="product-name">\n')),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Write the Crawling Code"),": Utilize your chosen web crawling tool to write code that navigates through the website, locates the desired data, and extracts it.\nThis involves sending HTTP requests, parsing HTML content, and selecting the relevant elements.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Handle Website-Specific Challenges"),": Some websites may implement anti-crawling measures like CAPTCHA or rate limiting.\nImplement strategies like rotating IP addresses or adding delays in your crawling code to handle such challenges."))),(0,r.kt)("h2",{id:"ethical-considerations"},"Ethical Considerations"),(0,r.kt)("p",null,"While web crawling can be a powerful tool for data extraction, it\u2019s important to respect website owners\u2019 terms of service and adhere to ethical guidelines. Always ensure that your crawling activities are legal and ethical.\nBe mindful of any website-specific crawling policies and consider reaching out to website owners for permission when necessary."),(0,r.kt)("h2",{id:"conclusion"},"Conclusion"),(0,r.kt)("p",null,"Web crawling opens up a world of possibilities for data extraction and analysis. By automating the process of gathering data from websites, you can save time and collect valuable insights. Armed with the knowledge from this beginner\u2019s guide, you\u2019re ready to embark on your web crawling journey.\nRemember to stay ethical, explore different tools, and continue learning as you dive deeper into the exciting world of web crawling."))}m.isMDXComponent=!0}}]);